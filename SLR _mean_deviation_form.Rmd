---
title: "Deviation around the mean formulation oi simple linear regression"
author: "by Craig W. Slinkman"
date: "12/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a great question.  I wish some of my past students were that inquisitive.

Suppose we are fitting the standard linear regression mode:

$$
\begin{equation}
    \tag{1}
    y_i = \beta_0 + \beta_1 x_i + e_i
\end{equation}$$

where the usual assumptions of linearity, independent error terms, and constant variance are satisfied.

Then the solution to the normal equations normal equations are given by:

$$
\tag{2}
\hat{\beta_1} = \frac{ \Sigma (y_i-\bar{y})\bar{x_i-\bar{x}})}{\Sigma (x_i-\bar{x})^2}
$$
and

$$
\tag{2b}
\hat{\beta_0} = \bar{y} - \hat{\beta_1 \bar{x}}
$$
The estimated regression equation is

$$
\begin{equation}
\tag{3}
\hat{y}_i = \beta_0 + \beta_1 x_i
\end{equation}
$$

where $\hat{y}_i$ are the fitted or predicted values. If we substitute caution (2b) into equation (3) to eliminate $\hat{\beta}_0$ and collect like terms we get 

\begin{align}
\hat{y}_1 & = \hat{\beta}_0 + \hat{\beta_1} x_1 \\
          & = ( \bar{y} - \hat{\beta_1} \bar{x} ) + \hat{\beta_1} x_1 \\
          & = \bar{y} - \hat{\beta_1} \bar{x} +  \hat{\beta_1} x_1 \\
          & = \bar{y} + \hat{\beta_1} (x_i - \bar{x} )
\end{align}          

We subtract $\bar{y}$ from both sides we get 

\begin{equation}
\tag{4}
(y_I - \bar{y}) = \hat{\beta}_1( x_1 - \bar{x} )
\end{equation}

I call equation (4) the deviation around the means formulation of the simple regression model.  Its interpenetration is that is the predictor variable is one $x_i$ unit above the mean then on average (recalling that estimated regression coefficients are means) on average the corresponding $y_i$ will be $\hat{\beta_1}$ above the mean.

What are the advantages of this formulation of simple linear regression.

1. Most individuals thin of hat{\beta_0} as the y-intercept.  That is it is the expected value of $y_i$ when $x_i=0$.  Most of the time this is an incorrect interpretation because most of the time the intercept is outside of the span of the predictor variable $x_1$ range of data.  We know the risk involved when extrapolate outside the range of the data because we have no data about the behavior of the model in this region.  In your case of the Capital Asses Priccing Model we know that the predictor variable $x_i$ has both negative and positive values so zero is included within the range of the predictor variable.

2.  The equation shows that the regression line passes through the point $( \bar{x},\bar{y})$

Note that the mean deviation model appears to have only one parameter to be estimated, that is, $\beta_0$.  This is not the case so the degrees of freedom for simple linear regression are still $n-2$ because the value of \beta_0$ is hidden by the substitution we made to derive the deviation around the means equation.